# Manus Prime: Multimodal Architecture Plan

## Overview
Extend **GPT-OSS-20B** to become a fully multimodal agentic model with vision, audio, video, RAG, and tool capabilities.

## Current Capabilities (GPT-OSS-20B)
| Capability | Native Support |
|------------|---------------|
| Text/Code Generation | ✅ Yes |
| Reasoning (Low/Med/High) | ✅ Yes |
| Tool/Function Calling | ✅ Yes |
| Web Browsing | ✅ Yes |
| Python Execution | ✅ Yes |
| **Vision/Image** | ❌ No |
| **Audio/Speech** | ❌ No |
| **Video** | ❌ No |
| **RAG** | ❌ No (external) |

---

## Proposed Modular Architecture

```
┌─────────────────────────────────────────────────────────────┐
│                    NEXUS PRIME CORE                         │
│                   (GPT-OSS-20B Base)                        │
├─────────────────────────────────────────────────────────────┤
│  ┌─────────┐  ┌─────────┐  ┌─────────┐  ┌─────────┐        │
│  │ Vision  │  │  Audio  │  │  Video  │  │   RAG   │        │
│  │ Encoder │  │ Encoder │  │ Encoder │  │ Module  │        │
│  │ (SigLIP)│  │(Whisper)│  │ (ViViT) │  │(Qdrant) │        │
│  └────┬────┘  └────┬────┘  └────┬────┘  └────┬────┘        │
│       │            │            │            │              │
│  ┌────┴────────────┴────────────┴────────────┴────┐        │
│  │           Multimodal Projector (MLP)           │        │
│  │         (Maps all modalities to LLM space)     │        │
│  └────────────────────────┬───────────────────────┘        │
│                           ▼                                 │
│  ┌─────────────────────────────────────────────────┐       │
│  │              GPT-OSS-20B (Frozen/LoRA)          │       │
│  │         + Tool Calling + Reasoning Levels       │       │
│  └─────────────────────────────────────────────────┘       │
│                           │                                 │
│  ┌────────────────────────┴────────────────────────┐       │
│  │              Output Heads                        │       │
│  ├─────────┬──────────┬──────────┬─────────────────┤       │
│  │  Text   │  Tools   │  Speech  │   Actions       │       │
│  │ (Token) │  (JSON)  │  (TTS)   │  (Browser/Py)   │       │
│  └─────────┴──────────┴──────────┴─────────────────┘       │
└─────────────────────────────────────────────────────────────┘
```

---

## Module Implementation Plan

### 1. Vision Module ⏱️ 2 days
| Component | Model | VRAM | Integration |
|-----------|-------|------|-------------|
| Vision Encoder | `google/siglip-so400m-patch14-384` | ~1.5GB | Frozen |
| Projector | 2-layer MLP | ~0.1GB | Trainable |
| **Total** | | **~1.6GB** | |

**Implementation**: LLaVA-style image token injection

### 2. Audio/Speech Module ⏱️ 2 days
| Component | Model | VRAM | Integration |
|-----------|-------|------|-------------|
| Speech-to-Text | `openai/whisper-large-v3-turbo` | ~1.5GB | Frozen |
| Text-to-Speech | `parler-tts/parler-tts-mini-v1` | ~0.5GB | Frozen |
| Projector | 1-layer MLP | ~0.05GB | Trainable |
| **Total** | | **~2GB** | |

**Implementation**: Audio embeddings → projector → LLM

### 3. Video Module ⏱️ 3 days
| Component | Model | VRAM | Integration |
|-----------|-------|------|-------------|
| Video Encoder | `google/vivit-b-16x2-kinetics400` | ~0.5GB | Frozen |
| Frame Sampler | Custom (8 frames) | CPU | - |
| Projector | 2-layer MLP | ~0.1GB | Trainable |
| **Total** | | **~0.6GB** | |

**Implementation**: Sample frames → ViViT → projector → LLM

### 4. RAG Module ⏱️ 1 day
| Component | Model | VRAM | Integration |
|-----------|-------|------|-------------|
| Embeddings | `BAAI/bge-m3` | ~0.5GB | Frozen |
| Vector DB | Qdrant (local) | CPU/RAM | External |
| Retriever | Custom | CPU | - |
| **Total** | | **~0.5GB** | |

**Implementation**: Query → embed → retrieve → inject context

### 5. Tool Calling ✅ Already Supported
GPT-OSS-20B has native support for:
- Function calling with schemas
- Web browsing
- Python code execution

---

## VRAM Budget (RTX 5080 16GB)

| Component | VRAM |
|-----------|------|
| GPT-OSS-20B (MXFP4) | ~10GB |
| Vision (SigLIP) | ~1.6GB |
| Audio (Whisper+TTS) | ~2GB |
| Video (ViViT) | ~0.6GB |
| RAG (BGE-M3) | ~0.5GB |
| LoRA Adapters | ~0.5GB |
| **TOTAL** | **~15.2GB** ✅ |

> [!WARNING]
> Tight fit! May need to load modules on-demand or use CPU offloading for some encoders.

---

## Training Strategy

### Phase 1: Base SFT (Current)
- Train GPT-OSS-20B on 1B "Manus Prime" text trajectories
- No multimodal yet

### Phase 2: Vision Alignment
- Add SigLIP encoder
- Train projector on vision-code pairs
- ~100k image-to-code samples

### Phase 3: Audio Integration  
- Add Whisper + TTS
- Train on voice-to-code tasks
- ~50k audio samples

### Phase 4: Video Understanding
- Add ViViT encoder
- Train on video analysis tasks
- ~20k video samples

### Phase 5: RAG Enablement
- Integrate Qdrant
- No training needed (retrieval is external)

---

## Decision Required

> [!IMPORTANT]
> **Choose implementation scope:**
>
> 1. **Phase 1 Only**: Pure text GPT-OSS-20B training
> 2. **Phase 1 + 2**: Add vision after base training
> 3. **Full Multimodal**: All 5 phases (2-3 weeks extra)

---

## Next Steps
1. [ ] User approval of architecture
2. [ ] Update [04_sft_training.py](file:///mnt/d/Research%20Experiments/manus_model/04_sft_training.py) for GPT-OSS-20B
3. [ ] Create modular encoder integration code
4. [ ] Generate multimodal training data (if needed)
