# Manus Prime: Full Multimodal I/O Architecture

## Overview
Build a **fully multimodal** model based on GPT-OSS-20B that can:
- **INPUT**: Text, Code, Image, Audio, Speech, Video
- **OUTPUT**: Text, Code, Image, Audio, Speech

---

## Complete Modality Matrix

| Modality | Input | Output | Encoder | Decoder |
|----------|-------|--------|---------|----------|
| **Text** | ✅ | ✅ | GPT-OSS tokenizer | GPT-OSS generation |
| **Code** | ✅ | ✅ | GPT-OSS tokenizer | GPT-OSS generation |
| **Image** | ✅ | ✅ | SigLIP | Stable Diffusion |
| **Audio** | ✅ | ✅ | Whisper encoder | AudioLM / CLAP |
| **Speech** | ✅ | ✅ | Whisper STT | Parler-TTS |
| **Video** | ✅ | ❌ | ViViT | (Complex - future) |

---

## Architecture Diagram

```
                    ┌───────────────────────────────────┐
                    │       INPUT ENCODERS          │
                    ├───────┬───────┬───────┬───────┬─────┤
                    │ Text  │ Image │ Audio │ Video │ Code│
                    │  ↓    │ SigLIP│Whisper│ ViViT │  ↓   │
                    └───┬───┴───┬───┴───┬───┴───┬───┴─┬───┘
                        │       │       │       │     │
                        └───────┴───┬───┴───────┴─────┘
                                    │
                    ┌───────────────┴───────────────┐
                    │  MULTIMODAL PROJECTOR (MLP)   │
                    │  Maps all modalities → LLM    │
                    └───────────────┬───────────────┘
                                    │
                    ┌───────────────┴───────────────┐
                    │     GPT-OSS-20B (Core LLM)    │
                    │  + Reasoning + Tool Calling   │
                    │  + 32K Context (RoPE Scaled)  │
                    └───────────────┬───────────────┘
                                    │
                    ┌───────────────┴───────────────┐
                    │   OUTPUT ROUTING HEAD         │
                    │  (Classifies output modality) │
                    └───────────────┬───────────────┘
                                    │
                    ┌───────────────┴───────────────┐
                    │       OUTPUT DECODERS         │
                    ├───────┬───────┬───────┬───────┬─────┤
                    │ Text  │ Image │ Audio │Speech │ Code│
                    │ Token │  SD   │AudioLM│  TTS  │Token│
                    └───────┴───────┴───────┴───────┴─────┘
```

---

## Components

### INPUT Encoders (Frozen)
| Encoder | Model | VRAM | Purpose |
|---------|-------|------|----------|
| Vision | `google/siglip-so400m-patch14-384` | 1.5GB | Image understanding |
| Audio | `openai/whisper-large-v3-turbo` | 1.5GB | Audio/speech input |
| Video | `google/vivit-b-16x2` | 0.5GB | Video frame analysis |

### OUTPUT Decoders
| Decoder | Model | VRAM | Purpose |
|---------|-------|------|----------|
| Image | `stabilityai/sdxl-turbo` | 3GB | Image generation |
| Speech | `parler-tts/parler-tts-mini-v1` | 0.5GB | Text-to-speech |
| Audio | `facebook/audiogen-medium` | 1GB | Audio generation |

### Trainable Components
| Component | Size | What We Train |
|-----------|------|---------------|
| Input Projectors | ~0.5GB | Map encoder → LLM space |
| Output Heads | ~0.3GB | Route LLM → decoder |
| LoRA Adapters | ~0.5GB | Fine-tune LLM |

---

## VRAM Budget (16GB RTX 5080)

| Component | VRAM |
|-----------|------|
| GPT-OSS-20B (MXFP4) | ~10GB |
| Input Encoders (frozen) | ~3.5GB |
| Output Decoders | ON-DEMAND* |
| Projectors + LoRA | ~1GB |
| **TOTAL (training)** | **~14.5GB** ✅ |

*Output decoders loaded on-demand during inference

---

## Datasets for Each Modality

### Input Training
| Modality | Dataset | Samples |
|----------|---------|----------|
| Image→Text | WebSight | 100K |
| Audio→Text | Common Voice | 10K |
| Video→Text | FineVideo | 1K |

### Output Training  
| Modality | Dataset | Samples |
|----------|---------|----------|
| Text→Image | LAION-Aesthetics | 50K |
| Text→Speech | MLS TTS | 10K |
| Text→Audio | AudioCaps | 10K |

---

## Training Phases

1. **Phase 1**: Text/Code only (Current 1B dataset)
2. **Phase 2**: Add image input (SigLIP projector)
3. **Phase 3**: Add audio/speech input (Whisper projector)
4. **Phase 4**: Add image output (SD adapter)
5. **Phase 5**: Add speech output (TTS adapter)
6. **Phase 6**: Add video input (ViViT projector)

---

## Decision Required

> [!IMPORTANT]
> **Choose implementation scope:**
>
> 1. **Phase 1 Only**: Pure text GPT-OSS-20B training
> 2. **Phase 1 + 2**: Add vision after base training
> 3. **Full Multimodal**: All 5 phases (2-3 weeks extra)

---

## Next Steps

1. [ ] User approval of architecture
2. [ ] Update [04_sft_training.py](file:///mnt/d/Research%20Experiments/manus_model/04_sft_training.py) for GPT-OSS-20B
3. [ ] Create modular encoder integration code
4. [ ] Generate multimodal training data (if needed)
