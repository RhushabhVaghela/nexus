# Dataset Regeneration with Native Schema Support

## Goal
Regenerate both datasets with:
1. **30+ domains** with equal weightage
2. **0 redundancy** (hash-based deduplication)
3. **Native tool-calling schema** (OpenAI-style `tool_calls`)
4. **Universal model compatibility** via `core/` module integration

---

## Current Architecture Gap

```
❌ Current Flow:
Generator → Legacy JSON-in-string format → Training

✅ New Flow:
Generator → core/model_config.py (detect model) → Native format → Training
```

The `core/` module has detection but generators don't use it yet.

---

## Proposed Changes

### 1. Create Universal Data Format Module

#### [NEW] `core/data_format.py`
Universal data formatting that auto-detects and outputs the correct schema:

```python
class UniversalDataFormatter:
    def __init__(self, model_name: str):
        self.model_config = UniversalModelConfig(model_name)
        self.use_native_tools = self.model_config.supports_tool_calling
    
    def format_trajectory(self, steps: List[Dict]) -> Dict:
        """Format trajectory in native or legacy format based on model."""
        if self.use_native_tools:
            return self._format_native(steps)
        else:
            return self._format_legacy(steps)
    
    def _format_native(self, steps) -> Dict:
        """OpenAI-style tool_calls format."""
        messages = []
        for step in steps:
            if step["type"] == "action":
                messages.append({
                    "role": "assistant",
                    "content": None,
                    "tool_calls": [{
                        "id": f"call_{uuid4().hex[:8]}",
                        "type": "function",
                        "function": {
                            "name": step["tool"],
                            "arguments": json.dumps(step["input"])
                        }
                    }]
                })
                messages.append({
                    "role": "tool",
                    "tool_call_id": messages[-1]["tool_calls"][0]["id"],
                    "content": step.get("output", "")
                })
        return {"messages": messages}
```

---

### 2. Update Finetuned Dataset Generator

#### [MODIFY] [01_generate_finetuned_dataset.py](file:///mnt/d/Research%20Experiments/manus_model/01_generate_finetuned_dataset.py)

**Changes:**
1. Import `UniversalDataFormatter` from `core/`
2. Expand `BLUEPRINT_LIBRARY` to 30+ domains with equal selection
3. Add hash-based deduplication
4. Output in native tool-calling format

**30+ Domains (Equal Weightage):**
```python
BLUEPRINT_LIBRARY = [
    # Developer Tools (6)
    "Manus Clone", "Web SQL Client", "Browser Music Sequencer",
    "Code Playground", "API Testing Tool", "Git Visualization",
    
    # Business Apps (6)
    "Enterprise CRM", "HR Management", "Invoice Generator",
    "Project Tracker", "Inventory System", "Meeting Scheduler",
    
    # Creative/Content (6)
    "Slide Deck", "Portfolio", "Modern Blog", "Podcast Platform",
    "Video Editor", "Design System Generator",
    
    # Finance (4)
    "Personal Finance Tracker", "Crypto Dashboard", "Tax Calculator",
    "Investment Portfolio",
    
    # Healthcare (3)
    "Telehealth Platform", "Fitness Tracker", "Mental Health Journal",
    
    # AI/ML (4)
    "Vision AI App", "Chatbot Builder", "ML Model Dashboard",
    "Data Annotation Tool",
    
    # E-commerce (3)
    "Online Store", "Product Catalog", "Checkout Flow",
    
    # Legal/Documents (2)
    "Legal Document Automation", "Contract Generator",
    
    # Social (2)
    "Social Feed", "Real-time Chat",
]  # Total: 36 domains
```

**Deduplication:**
```python
class DeduplicatedGenerator:
    def __init__(self):
        self.seen_hashes = set()
    
    def is_duplicate(self, sample: Dict) -> bool:
        h = hashlib.md5(json.dumps(sample, sort_keys=True).encode()).hexdigest()
        if h in self.seen_hashes:
            return True
        self.seen_hashes.add(h)
        return False
```

---

### 3. Update Repetitive Dataset Generator

#### [MODIFY] [03_generate_repetitive_dataset.py](file:///mnt/d/Research%20Experiments/manus_model/03_generate_repetitive_dataset.py)

**Changes:**
1. Use native tool-calling format for tool-based queries
2. Add more procedural generators (10+ techniques)
3. Hash-based deduplication

**Generators to Add:**
```python
GENERATORS = [
    gen_log_extraction,      # Existing
    gen_json_lookup,         # Existing
    gen_directory_lookup,    # Existing
    gen_table_lookup,        # Added in resume
    gen_config_parsing,      # NEW: Parse YAML/TOML configs
    gen_regex_extraction,    # NEW: Extract patterns
    gen_date_calculation,    # NEW: Date math
    gen_unit_conversion,     # NEW: Convert units
    gen_code_completion,     # NEW: Complete code snippets
    gen_error_diagnosis,     # NEW: Diagnose from stack traces
]
```

---

### 4. Update Training Scripts

#### [MODIFY] [06_sft_training.py](file:///mnt/d/Research%20Experiments/manus_model/06_sft_training.py)

Use chat template from `core/` to ensure consistency:

```python
from core import UniversalModelConfig, ChatTemplateRegistry, detect_template

# Auto-detect model capabilities
model_config = UniversalModelConfig(CONFIG["model_name"])
print(f"Model supports native tools: {model_config.supports_tool_calling}")

# Use appropriate chat template
template = ChatTemplateRegistry.from_tokenizer(tokenizer)
```

---

## Execution Plan

### Phase 1: Update Core Module (30 min)
- [ ] Create `core/data_format.py` with `UniversalDataFormatter`
- [ ] Update [core/__init__.py](file:///mnt/d/Research%20Experiments/manus_model/core/__init__.py) to export new class

### Phase 2: Update Finetuned Generator (1 hour)
- [ ] Expand `BLUEPRINT_LIBRARY` to 36 domains
- [ ] Implement equal-weight random selection
- [ ] Add `DeduplicatedGenerator` class
- [ ] Integrate `UniversalDataFormatter`
- [ ] Update output to use native tool schema

### Phase 3: Update Repetitive Generator (45 min)
- [ ] Add 6 new procedural generators
- [ ] Add deduplication
- [ ] Format output for native schema

### Phase 4: Start Regeneration (Background)
- [ ] Generate finetuned dataset: 1B samples (~10 hours)
- [ ] Generate repetitive dataset: 200M samples (~8 hours)

### Phase 5: Validation
- [ ] Run validators on new datasets
- [ ] Verify 0 redundancy
- [ ] Verify native schema format

---

## Expected Output Format

### Finetuned (Native Tool Schema)
```json
{
  "messages": [
    {"role": "user", "content": "Build a Portfolio using React..."},
    {
      "role": "assistant",
      "content": null,
      "tool_calls": [{
        "id": "call_abc123",
        "type": "function",
        "function": {
          "name": "code_editor",
          "arguments": "{\"file\": \"App.jsx\", \"action\": \"create\"}"
        }
      }]
    },
    {"role": "tool", "tool_call_id": "call_abc123", "content": "Created App.jsx"},
    {"role": "assistant", "content": "I've created the React app..."}
  ],
  "domain": "portfolio",
  "blueprint": "Portfolio",
  "has_failure_recovery": false
}
```

### Repetitive (Native Schema)
```json
{
  "messages": [
    {"role": "user", "content": "Find employee X's phone..."},
    {"role": "assistant", "content": "Extension: 4521"}
  ],
  "domain": "repetitive_prompting",
  "technique": "directory_lookup"
}
```

---

## Disk Space Requirements

| Dataset | Samples | Est. Size |
|---------|---------|-----------|
| Finetuned | 1B | ~250GB |
| Repetitive | 200M | ~80GB |
| **Total** | 1.2B | ~330GB |

E: drive has sufficient space for both.
