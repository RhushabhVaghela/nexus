# Manus Prime: Data Pipeline Verification & Walkthrough

> [!IMPORTANT]
> The data generation pipeline has been enhanced to "Manus Prime" standards. It now produces rich, agentic trajectories with simulated resiliency (failure & recovery) across 26+ diverse verticals.

## 1. System Architecture Upgrade

The entire pipeline was refactored to support a strict **JSON-based Chain of Thought (CoT)** format, replicating the [manus_training_data.jsonl](file:///mnt/d/Research%20Experiments/manus_model/others-stuff-not-required/manus_training_data.jsonl) reference schema.

| component | Status | Upgrade Details |
| :--- | :--- | :--- |
| **Generator** | ✅ Active | Implements "No Limits" blueprints (26+ types) + "Failure & Recovery" logic. |
| **Validator** | ✅ Updated | Recursively validates `datasets/` for Schema compliance (Messages List + JSON CoT). |
| **SFT Trainer** | ✅ Refactored | Uses `messages` format with Domain-Specific System Prompts. |
| **Rejection Sampler** | ✅ Refactored | Prompts for JSON output and parses structured responses for scoring. |

## 2. Capability Showcase

### A. Agentic Resilience (Failure & Recovery)

30% of generated samples now simulate a failure scenario to train the model in error recovery.

```json
// Example Step Sequence from Generator
[
  { "step": 1, "thought": "Initializing project..." },
  { 
    "step": 2, 
    "action": "npm install legacy-lib", 
    "failure": "Error: Dependency conflict detected..." 
  },
  { 
    "step": 3, 
    "recovery": "I will resolve by upgrading to v2.0...", 
    "action": "npm install legacy-lib@2.0" 
  }
]
```

### B. "No Limits" Domain Coverage

We are generating specialized trajectories for:

- **FinTech**: Crypto Dashboards, Finance Trackers
- **Health**: Bioinformatics, Telehealth
- **Enterprise**: CRM, Legal Automation
- **Creative**: Generative Art, Music Sequencers
- **System Design**: Twitter Clones, Load Balancers

## 3. Usage & Verification

### Running the Pipeline

The [run_full_pipeline.sh](file:///mnt/d/Research%20Experiments/manus_model/run_full_pipeline.sh) script currently orchestrates the old flow. You can run components individually for verification:

1. **Check Generation Progress**:

   ```bash
   tail -f logs/gen_1B_manus_prime.log
   ```

2. **Validate New Data**:

   ```bash
   python3 03_validate_trajectories.py
   ```

   *Output: [datasets/train/part_000_validated.jsonl](file:///mnt/d/Research%20Experiments/manus_model/datasets/train/part_000_validated.jsonl)*

3. **Start Training (SFT)**:

   ```bash
   python3 04_sft_training.py
   ```

   *Note: Ensure validation is run first.*

## 4. Complete Pipeline Execution

### **Option A: Unified Pipeline (Recommended)**
Run the complete end-to-end pipeline:
```bash
bash run_complete_pipeline.sh
```

This orchestrates:
1. Data Generation (1B samples)
2. Validation
3. SFT Training 
4. Rejection Sampling (optional)
5. GRPO Training (optional)

### **Option B: Training-Only Pipeline**
If data generation is already complete:
```bash
bash run_training_pipeline.sh
```

### **Option C: Individual Scripts**
Run stages manually:
```bash
python3 02_generate_trajectories.py  # Generation
python3 03_validate_trajectories.py  # Validation
python3 04_sft_training.py           # SFT
python3 05_rejection_sampling.py     # Rejection (optional)
python3 06_grpo_training.py          # GRPO (optional)
```

---

## 5. Pause/Resume Controls

### **Supported Stages:**
| Stage | Pause Support | Method |
|-------|--------------|--------|
| Data Generation | ❌ No | Let it complete (~10h) |
| Validation | ❌ No | Fast (~3h) |
| **SFT Training** | **✅ YES** | Trainer Callback |
| **Rejection Sampling** | **✅ YES** | Progress Save |
| **GRPO Training** | **✅ YES** | Trainer Callback |

### **How to Control Training:**
While any training stage is running, open a new terminal:
```bash
python3 utils/control_training.py --flag-dir flags
```

**Commands:**
- `p` : **Pause** (Save checkpoint and exit safely)
- `r` : **Resume** (Clear flags; restart the script to continue)
- `n` : **Next** (Stop immediately without saving)
- `q` : **Quit** controller

**Resume After Pause:**
Simply re-run the same script - it will automatically detect and resume from the last checkpoint:
```bash
python3 04_sft_training.py  # Resumes from checkpoint-XXX
```

---

## 6. Next Steps

- Monitor the 1 Billion sample generation.
- Launch SFT once sufficient data (~100k samples) is accumulated.
